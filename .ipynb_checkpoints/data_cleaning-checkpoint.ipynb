{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spotify Song Data - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data cleaning notebook for a classification project that uses a dataset of Spotify song data to determine what features make a song popular on the platform, aka a hit song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-22T01:24:50.065028Z",
     "start_time": "2021-03-22T01:24:48.383956Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/Users/spags/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# Regulars\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm\n",
    "# %%capture\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(df):\n",
    "    '''Simple function that takes in a full dataframe and returns\n",
    "    some basic information about the countents of the dataframe.'''\n",
    "    \n",
    "    print('Shape of DataFrame:\\n', df.shape)\n",
    "    print('\\nDataFrame Info:')\n",
    "    print(df.info())\n",
    "    print('\\n Null Values Present:\\n', df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spotify_song_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking out some info\n",
    "get_info(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the song name as the index for the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('name', inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suppose we can drop the ID column as it's not really necessary for this project at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis = 1, inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking nulls \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nulls here, so we're in the clear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the release date to a datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different approach here. At first, we changed the values of the year column to strings so that they wouldn't be affected by scaling. The drawback here is that we wound up with a massive amount of columns after one hot encoding it as a categorical column. Let's try binning year into decades to cut down on these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(df['year']):\n",
    "    if i >= 1920 and i < 1930:\n",
    "        df['year'] = df['year'].replace(i, '1920s')\n",
    "    elif i >= 1930 and i < 1940:\n",
    "        df['year'] = df['year'].replace(i, '1930s')\n",
    "    elif i >= 1940 and i < 1950:\n",
    "        df['year'] = df['year'].replace(i, '1940s')\n",
    "    elif i >= 1950 and i < 1960:\n",
    "        df['year'] = df['year'].replace(i, '1950s')\n",
    "    elif i >= 1960 and i < 1970:\n",
    "        df['year'] = df['year'].replace(i, '1960s')\n",
    "    elif i >= 1970 and i < 1980:\n",
    "        df['year'] = df['year'].replace(i, '1970s')\n",
    "    elif i >= 1980 and i < 1990:\n",
    "        df['year'] = df['year'].replace(i, '1980s')\n",
    "    elif i >= 1990 and i < 2000:\n",
    "        df['year'] = df['year'].replace(i, '1990s')\n",
    "    elif i >= 2000 and i < 2010:\n",
    "        df['year'] = df['year'].replace(i, '2000s')\n",
    "    elif i >= 2010 and i < 2020:\n",
    "        df['year'] = df['year'].replace(i, '2010s')\n",
    "    elif i >= 2020 and i < 2029:\n",
    "        df['year'] = df['year'].replace(i, '2020s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artists'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping all the artists in the artist column creates an issue when one hot encoding...namely that it expands to over 31k columns and creates an issue for running our models. Let's do some exploration here and see if there's any way to bin artists in a sensible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['artists'].value_counts().unique().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow...31k+ unique artists represented in this dataset. This presents an interesting dilemma. It doesn't seem too likely that we can figure out a way to bin this column in a sensible way. We could separate the artists into bins of multiple artists and single artists for the tracks but that doesn't really take the individual artist popularity into account.\n",
    "\n",
    "For now, we're going to have to dump the artist column and then revisit this down the line after being able to get a few successful models running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('artists', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is going to take some thought and research. We know that the popularity metric is a number from 1 to 100 that (with 100 being the most popular) that is assigned to a song to denote it's popularity. Spotify calculates this metric based on total streams, trends, and several other factors. First, we need to see what we're working with in terms of the value counts. Next, we need to make some sort of determination of what level of popularity constitutes as a hit song and the level that constitutes a dud.\n",
    "\n",
    "<b> Note:</b> In the other notebook, we work on a multiclass version that creates 3 targets: hit, solid single, and dud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder Plot\n",
    "\n",
    "ax = plt.figure(figsize = (24, 6))\n",
    "ax = sns.countplot(df['popularity'])\n",
    "ax.set_title('Song Popularity Countplot')\n",
    "ax.set_xlabel('Popularity')\n",
    "ax.set_ylabel('Count')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see from the figure above that an overhwelmingly large percentage of the songs have a popularity of 0 and the top of the scale is an extremely small percentage.  This is going to wreak havok on class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>From Spotify:</b><br>\n",
    "The popularity of the track. The value will be between 0 and 100, with 100 being the most popular. The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are. Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity. Note that the popularity value may lag actual popularity by a few days: the value is not updated in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make the following labels by binning hte popularity metric:<br>\n",
    "0 - Not a hit<br>\n",
    "1 - Hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the following code with various values to check what our thresholds should be\n",
    "\n",
    "df[df[\"popularity\"] == 65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we'll create strings and then label encode from there.  I'm sure there's an easier way, but this is what's worked for me to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(df['popularity']):\n",
    "    if i >= 65:\n",
    "        df['popularity'] = df['popularity'].replace(i, 'Hit')\n",
    "    else:\n",
    "        df['popularity'] = df['popularity'].replace(i, 'Dud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['popularity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for encoding it for 1 and 0\n",
    "\n",
    "for i in tqdm(df['popularity']):\n",
    "    if i == 'Hit':\n",
    "        df['popularity'] = df['popularity'].replace(i, 1)\n",
    "    else: \n",
    "        df['popularity'] = df['popularity'].replace(i, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['popularity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Clean DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've successfully cleaned the data, we can save our clean dataframe to use in other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('clean_spotify_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
